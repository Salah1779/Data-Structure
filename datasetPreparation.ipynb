{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxn7YxhSzMEtbl9/IUnknv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Salah1779/Data-Structure/blob/master/datasetPreparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ_bkuERQuWX",
        "outputId": "912af672-7ebc-41f1-f6a1-0b5d000e6079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "385\n",
            "385\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data with 100 designations in multiple languages\n",
        "data = {\n",
        "    'id_category': [\n",
        "        # Food & Drinks\n",
        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "\n",
        "        # Vehicle\n",
        "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "        2, 2, 2, 2, 2,\n",
        "        # Housing\n",
        "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
        "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
        "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
        "        3, 3, 3, 3, 3,\n",
        "        # Transportation\n",
        "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
        "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
        "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
        "        4, 4, 4, 4, 4,\n",
        "        # Health\n",
        "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
        "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
        "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
        "        5,5, 5, 5, 5,\n",
        "        # Entertainment\n",
        "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
        "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
        "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
        "        6, 6, 6, 6, 6,\n",
        "        # Investment\n",
        "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
        "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
        "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
        "        7, 7, 7, 7, 7,\n",
        "        # Other\n",
        "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
        "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
        "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
        "        8, 8, 8, 8, 8,\n",
        "        # Financial Expenses\n",
        "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
        "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
        "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
        "        9, 9, 9, 9, 9,\n",
        "        # Shopping\n",
        "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
        "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
        "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
        "        10, 10, 10, 10, 10,\n",
        "\n",
        "    ] ,\n",
        "    'expense_designation': [\n",
        "        # Food & Drinks\n",
        "        'Fresh Milk 1L', 'Sliced Bread', 'Takeaway Sushi', 'Fresh Juice', 'Pasta Pack',\n",
        "        'Pizza Delivery', 'Cereal Box', 'Chocolate Bar', 'Coffee Pack', 'Eggs Dozen',\n",
        "\n",
        "        # French\n",
        "        'Lait frais 1L', 'Pain de mie', 'Plat à emporter Sushi', 'Jus frais', 'Paquet de pâtes',\n",
        "        'Livraison de pizza', 'Boîte de céréales', 'Barre chocolatée', 'Paquet de café', 'Oeufs douzaine',\n",
        "\n",
        "        # Arabic\n",
        "        'حليب طازج 1 لتر', 'خبز شرائح', 'سوشي جاهز', 'عصير طازج', 'باقة مكرونة',\n",
        "        'بيتزا توصيل', 'علبة حبوب', 'لوح شوكولاتة', 'علبة قهوة', 'بيضة دزينة',\n",
        "\n",
        "        # Spanish\n",
        "        'Leche fresca 1L', 'Pan de molde', 'Sushi para llevar', 'Jugo fresco', 'Paquete de pasta',\n",
        "        'Entrega de pizza', 'Caja de cereales', 'Barra de chocolate', 'Paquete de café', 'Docena de huevos',\n",
        "\n",
        "        # German\n",
        "        'Frische Milch 1L', 'Toastbrot', 'Sushi zum Mitnehmen', 'Fruchtsaft', 'Pasta-Pack',\n",
        "        'Pizza-Lieferung', 'Müsli-Schachtel', 'Schokoladenriegel', 'Kaffeepaket', 'Dutzend Eier',\n",
        "\n",
        "        # Italian\n",
        "        'Latte fresco 1L', 'Pane a fette', 'Sushi da asporto', 'Succo fresco', 'Pacco di pasta',\n",
        "        'Consegna di pizza', 'Scatola di cereali', 'Barretta di cioccolato', 'Pacchetto di caffè', 'Dozzina di uova',\n",
        "\n",
        "        # Chinese\n",
        "        '新鲜牛奶1升', '切片面包', '外卖寿司', '鲜榨果汁', '意大利面包装',\n",
        "        '披萨外送', '谷物盒', '巧克力棒', '咖啡包装', '一打鸡蛋',\n",
        "\n",
        "        # Vehicle\n",
        "        'Car Maintenance', 'Fuel Cost', 'Tire Change', 'Car Insurance', 'Public Transport Pass',\n",
        "\n",
        "        # French\n",
        "        'Entretien de voiture', 'Coût du carburant', 'Changer les pneus', 'Assurance voiture', 'Pass de transport public',\n",
        "\n",
        "        # Arabic\n",
        "        'صيانة السيارة', 'تكلفة الوقود', 'تغيير الإطارات', 'تأمين السيارة', 'بطاقة النقل العام',\n",
        "\n",
        "        # Spanish\n",
        "        'Mantenimiento del coche', 'Costo de combustible', 'Cambio de neumáticos', 'Seguro de automóvil', 'Pase de transporte público',\n",
        "\n",
        "        # German\n",
        "        'Auto Wartung', 'Benzinkosten', 'Reifenwechsel', 'Autoversicherung', 'Öffentlicher Verkehr Pass',\n",
        "\n",
        "        # Italian\n",
        "        'Manutenzione auto', 'Costo carburante', 'Cambio gomme', 'Assicurazione auto', 'Pass per trasporto pubblico',\n",
        "\n",
        "        # Chinese\n",
        "        '汽车维护', '燃油费用', '换轮胎', '汽车保险', '公共交通通行证',\n",
        "\n",
        "        # Housing\n",
        "        'Electricity Bill', 'Rent Payment', 'Home Insurance', 'Water Bill', 'Property Tax',\n",
        "\n",
        "        # French\n",
        "        'Facture d\\'électricité', 'Paiement de loyer', 'Assurance habitation', 'Facture d\\'eau', 'Taxe foncière',\n",
        "\n",
        "        # Arabic\n",
        "        'فاتورة الكهرباء', 'دفع الإيجار', 'تأمين المنزل', 'فاتورة الماء', 'ضريبة الممتلكات',\n",
        "\n",
        "        # Spanish\n",
        "        'Factura de electricidad', 'Pago de alquiler', 'Seguro de hogar', 'Factura de agua', 'Impuesto sobre la propiedad',\n",
        "\n",
        "        # German\n",
        "        'Stromrechnung', 'Mietzahlung', 'Hausversicherung', 'Wasserrechnung', 'Grundsteuer',\n",
        "\n",
        "        # Italian\n",
        "        'Bollette elettriche', 'Pagamento dell\\'affitto', 'Assicurazione casa', 'Bollette dell\\'acqua', 'Imposta sulla proprietà',\n",
        "\n",
        "        # Chinese\n",
        "        '电费账单', '租金支付', '房屋保险', '水费账单', '财产税',\n",
        "\n",
        "        # Transportation\n",
        "        'Monthly Transport Card', 'Train Ticket', 'Taxi Fare', 'Bus Ticket', 'Bicycle Maintenance',\n",
        "\n",
        "        # French\n",
        "        'Carte de transport mensuelle', 'Billet de train', 'Tarif taxi', 'Billet de bus', 'Entretien de vélo',\n",
        "\n",
        "        # Arabic\n",
        "        'بطاقة النقل الشهرية', 'تذكرة قطار', 'أجرة تاكسي', 'تذكرة حافلة', 'صيانة الدراجة',\n",
        "\n",
        "        # Spanish\n",
        "        'Tarjeta de transporte mensual', 'Boleto de tren', 'Tarifa de taxi', 'Boleto de autobús', 'Mantenimiento de bicicleta',\n",
        "\n",
        "        # German\n",
        "        'Monatskarte für den Verkehr', 'Zugticket', 'Taxitarif', 'Busfahrkarte', 'Fahrradwartung',\n",
        "\n",
        "        # Italian\n",
        "        'Carta dei trasporti mensile', 'Biglietto del treno', 'Tariffa del taxi', 'Biglietto dell\\'autobus', 'Manutenzione della bicicletta',\n",
        "\n",
        "        # Chinese\n",
        "        '月票', '火车票', '出租车费用', '公交车票', '自行车维护',\n",
        "\n",
        "        # Health\n",
        "        'Doctor Consultation', 'Health Insurance', 'Gym Membership', 'Medicine Purchase', 'Health Check-up',\n",
        "\n",
        "        # French\n",
        "        'Consultation chez le médecin', 'Assurance santé', 'Abonnement à la salle de sport', 'Achat de médicaments', 'Contrôle de santé',\n",
        "\n",
        "        # Arabic\n",
        "        'استشارة طبية', 'تأمين صحي', 'عضوية صالة الألعاب الرياضية', 'شراء أدوية', 'فحص صحي',\n",
        "\n",
        "        # Spanish\n",
        "        'Consulta médica', 'Seguro de salud', 'Membresía de gimnasio', 'Compra de medicinas', 'Chequeo médico',\n",
        "\n",
        "        # German\n",
        "        'Arzttermin', 'Krankenversicherung', 'Fitnessstudio-Mitgliedschaft', 'Medikamenten Einkauf', 'Gesundheitsuntersuchung',\n",
        "\n",
        "        # Italian\n",
        "        'Visita medica', 'Assicurazione sanitaria', 'Abbonamento in palestra', 'Acquisto di medicinali', 'Controllo sanitario',\n",
        "\n",
        "        # Chinese\n",
        "        '医生咨询', '健康保险', '健身房会员', '药物购买', '健康检查',\n",
        "\n",
        "        # Entertainment\n",
        "        'Cinema Ticket', 'Concert Ticket', 'Theater Play', 'Online Streaming Subscription', 'Video Game Purchase',\n",
        "\n",
        "        # French\n",
        "        'Billet de cinéma', 'Billet de concert', 'Pièce de théâtre', 'Abonnement de streaming', 'Achat de jeu vidéo',\n",
        "\n",
        "        # Arabic\n",
        "        'تذكرة سينما', 'تذكرة حفلة', 'مسرحية', 'اشتراك في البث المباشر', 'شراء لعبة فيديو',\n",
        "\n",
        "        # Spanish\n",
        "        'Boleto de cine', 'Boleto de concierto', 'Obra de teatro', 'Suscripción de streaming', 'Compra de videojuegos',\n",
        "\n",
        "        # German\n",
        "        'Kinokarte', 'Konzertkarte', 'Theaterstück', 'Streaming-Abonnement', 'Videospielkauf',\n",
        "\n",
        "        # Italian\n",
        "        'Biglietto del cinema', 'Biglietto del concerto', 'Spettacolo teatrale', 'Abbonamento di streaming', 'Acquisto di videogiochi',\n",
        "\n",
        "        # Chinese\n",
        "        '电影票', '音乐会票', '戏剧', '在线流媒体订阅', '购买视频游戏',\n",
        "\n",
        "        # Investment\n",
        "        'Stock Purchase', 'Bond Investment', 'Real Estate Investment', 'Retirement Fund Contribution', 'Savings Account Deposit',\n",
        "\n",
        "        # French\n",
        "        'Achat d\\'actions', 'Investissement obligataire', 'Investissement immobilier', 'Contribution au fonds de retraite', 'Dépôt sur un compte d\\'épargne',\n",
        "\n",
        "        # Arabic\n",
        "        'شراء الأسهم', 'استثمار السندات', 'استثمار عقاري', 'مساهمة في صندوق التقاعد', 'إيداع حساب التوفير',\n",
        "\n",
        "        # Spanish\n",
        "        'Compra de acciones', 'Inversión en bonos', 'Inversión en bienes raíces', 'Contribución al fondo de jubilación', 'Depósito en cuenta de ahorros',\n",
        "\n",
        "        # German\n",
        "        'Aktienkauf', 'Anleiheinvestition', 'Immobilieninvestition', 'Rentenfondsbeitrag', 'Einschuss auf ein Sparkonto',\n",
        "\n",
        "        # Italian\n",
        "        'Acquisto di azioni', 'Investimento in obbligazioni', 'Investimento immobiliare', 'Contributo al fondo pensione', 'Deposito su un conto di risparmio',\n",
        "\n",
        "        # Chinese\n",
        "        '股票购买', '债券投资', '房地产投资', '退休基金贡献', '储蓄账户存款',\n",
        "\n",
        "        # Other\n",
        "        'Gifts', 'Donations', 'Education Fees', 'Legal Fees', 'Unexpected Expenses',\n",
        "\n",
        "        # French\n",
        "        'Cadeaux', 'Dons', 'Frais d\\'éducation', 'Frais juridiques', 'Dépenses imprévues',\n",
        "\n",
        "        # Arabic\n",
        "        'هدايا', 'تبرعات', 'رسوم تعليمية', 'رسوم قانونية', 'نفقات غير متوقعة',\n",
        "\n",
        "        # Spanish\n",
        "        'Regalos', 'Donaciones', 'Cuotas educativas', 'Honorarios legales', 'Gastos imprevistos',\n",
        "\n",
        "        # German\n",
        "        'Geschenke', 'Spenden', 'Bildungsgebühren', 'Rechtskosten', 'Unvorhergesehene Ausgaben',\n",
        "\n",
        "        # Italian\n",
        "        'Regali', 'Donazioni', 'Spese per l\\'istruzione', 'Spese legali', 'Spese impreviste',\n",
        "\n",
        "        # Chinese\n",
        "        '礼物', '捐赠', '教育费用', '法律费用', '意外支出',\n",
        "\n",
        "        # Financial Expenses\n",
        "        'Bank Fees', 'Credit Card Payment', 'Loan Payment', 'Insurance Premium', 'Financial Advisor Fee',\n",
        "\n",
        "        # French\n",
        "        'Frais bancaires', 'Paiement par carte de crédit', 'Paiement de prêt', 'Prime d\\'assurance', 'Honoraires de conseiller financier',\n",
        "\n",
        "        # Arabic\n",
        "        'رسوم بنكية', 'دفع بطاقة ائتمان', 'دفع قرض', 'قسط التأمين', 'رسوم المستشار المالي',\n",
        "\n",
        "        # Spanish\n",
        "        'Comisiones bancarias', 'Pago con tarjeta de crédito', 'Pago de préstamo', 'Prima de seguro', 'Honorarios de asesor financiero',\n",
        "\n",
        "        # German\n",
        "        'Bankgebühren', 'Kreditkartenzahlung', 'Darlehenszahlung', 'Versicherungsprämie', 'Honorar für Finanzberater',\n",
        "\n",
        "        # Italian\n",
        "        'Commissioni bancarie', 'Pagamento con carta di credito', 'Pagamento del prestito', 'Premio assicurativo', 'Tariffe per consulente finanziario',\n",
        "\n",
        "        # Chinese\n",
        "        '银行费用', '信用卡支付', '贷款支付', '保险费', '财务顾问费用',\n",
        "\n",
        "        # Shopping\n",
        "        'Clothing Purchase', 'Online Shopping', 'Gift Purchase', 'Home Appliances Purchase',\n",
        "         'Shoes' , 'T-shirt' , 'Samsung TV ','mobile' , 'Clothes' ,'Laundry machine',\n",
        "\n",
        "        # French\n",
        "        'Achat de vêtements', 'Courses alimentaires', 'Achats en ligne', 'Achat de cadeaux', 'Achat d\\'appareils électroménagers',\n",
        "\n",
        "        # Arabic\n",
        "        'شراء ملابس', 'تسوق البقالة', 'التسوق عبر الإنترنت', 'شراء هدايا', 'شراء الأجهزة المنزلية',\n",
        "\n",
        "        # Spanish\n",
        "        'Compra de ropa', 'Compras de supermercado', 'Compras en línea', 'Compra de regalos', 'Compra de electrodomésticos',\n",
        "\n",
        "        # German\n",
        "        'Bekleidungsankauf', 'Lebensmitteleinkauf', 'Online-Einkauf', 'Geschenkekauf', 'Kauf von Haushaltsgeräten',\n",
        "\n",
        "        # Italian\n",
        "        'Acquisto di abbigliamento', 'Spesa', 'Acquisti online', 'Acquisto di regali', 'Acquisto di elettrodomestici',\n",
        "    ]\n",
        "}\n",
        "print(len(data['id_category']))\n",
        "print(len(data['expense_designation']))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gszd7sdKZNcj",
        "outputId": "c2f134f8-adee-4aa7-a715-b531e439ec3a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_category           expense_designation\n",
            "0              1                 Fresh Milk 1L\n",
            "1              1                  Sliced Bread\n",
            "2              1                Takeaway Sushi\n",
            "3              1                   Fresh Juice\n",
            "4              1                    Pasta Pack\n",
            "..           ...                           ...\n",
            "380           10     Acquisto di abbigliamento\n",
            "381           10                         Spesa\n",
            "382           10               Acquisti online\n",
            "383           10            Acquisto di regali\n",
            "384           10  Acquisto di elettrodomestici\n",
            "\n",
            "[385 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)  # Shuffle all rows\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgXimXVNf23r",
        "outputId": "a0c57a6c-e639-417d-b89e-56ae06702523"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_category     expense_designation\n",
            "0              2        Costo carburante\n",
            "1              2            Cambio gomme\n",
            "2              1               عصير طازج\n",
            "3              2           تأمين السيارة\n",
            "4              1              بيضة دزينة\n",
            "..           ...                     ...\n",
            "380            7   Immobilieninvestition\n",
            "381            6  Biglietto del concerto\n",
            "382            1                    切片面包\n",
            "383            8               Donations\n",
            "384            1          Pizza Delivery\n",
            "\n",
            "[385 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plGWN6YJghUB",
        "outputId": "cc641a34-d758-4f36-df10-94e26865a888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "metadata": {
        "id": "XT7_Hwm6kyMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your DataFrame with 'expense_designation' and 'id_category' columns\n",
        "# For example, you can load a CSV file like this:\n",
        "# df = pd.read_csv('your_file.csv')\n",
        "\n",
        "# Adjust labels to be zero-indexed\n",
        "df['id_category'] = df['id_category'] - 1  # Ensure labels start from 0\n",
        "\n",
        "# Display the DataFrame to check if it's loaded correctly\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czWdPcIKk-GP",
        "outputId": "ea0e4f2d-a3c4-4abc-e6ee-a60f305630cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_category                expense_designation\n",
            "0              4        عضوية صالة الألعاب الرياضية\n",
            "1              2                   Hausversicherung\n",
            "2              0                               咖啡包装\n",
            "3              1                 Assicurazione auto\n",
            "4              8        Pago con tarjeta de crédito\n",
            "..           ...                                ...\n",
            "375            3                         Bus Ticket\n",
            "376            4                       استشارة طبية\n",
            "377            9  Achat d'appareils électroménagers\n",
            "378            9                     Compra de ropa\n",
            "379            2                        Mietzahlung\n",
            "\n",
            "[380 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Tokenize the text data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "max_length = 128  # You can adjust this\n",
        "\n",
        "# Tokenize the texts\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in df['expense_designation']:\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "    input_ids.append(encoding['input_ids'])\n",
        "    attention_masks.append(encoding['attention_mask'])\n",
        "\n",
        "# Convert to tensors\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(df['id_category'].values)\n",
        "\n",
        "# Display shapes to confirm\n",
        "print(f'Input IDs Shape: {input_ids.shape}')\n",
        "print(f'Attention Masks Shape: {attention_masks.shape}')\n",
        "print(f'Labels Shape: {labels.shape}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZqZl_HAmrco",
        "outputId": "47e21bbb-6885-4550-df06-391ca78bc9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs Shape: torch.Size([380, 128])\n",
            "Attention Masks Shape: torch.Size([380, 128])\n",
            "Labels Shape: torch.Size([380])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Split the dataset into training and validation sets\n",
        "X_train_ids, X_val_ids, X_train_masks, X_val_masks, y_train, y_val = train_test_split(\n",
        "    input_ids, attention_masks, labels, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Display shapes to confirm\n",
        "print(f'Train Input IDs Shape: {X_train_ids.shape}')\n",
        "print(f'Validation Input IDs Shape: {X_val_ids.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3OFh66CniK6",
        "outputId": "e3356c3b-f76f-4f56-8a5f-d0aff77e640f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Input IDs Shape: torch.Size([342, 128])\n",
            "Validation Input IDs Shape: torch.Size([38, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create DataLoaders\n",
        "train_dataset = TensorDataset(X_train_ids, X_train_masks, y_train)\n",
        "val_dataset = TensorDataset(X_val_ids, X_val_masks, y_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# Display the number of batches\n",
        "print(f'Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syqkIKZKnoA8",
        "outputId": "80bfbacf-7bc3-44f4-8eba-b3d1360b7748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 22, Validation batches: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Initialize the model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(set(df['id_category'])))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhvNYiWCnrg-",
        "outputId": "e08ecf62-5435-411e-8700-b12a5d06a4ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2NI9UVMnwba",
        "outputId": "03d7cd65-1748-4f83-ec60-98505195454e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Train the model\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate predictions\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = total_correct / total_samples * 100  # Convert to percentage\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4omMKERrVZl",
        "outputId": "d01b8940-533f-4148-81b1-32df2a7bd47a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 2.2397, Accuracy: 17.84%\n",
            "Epoch 2/3, Loss: 1.8046, Accuracy: 50.29%\n",
            "Epoch 3/3, Loss: 1.2722, Accuracy: 73.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Evaluate the model\n",
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "        true_labels.append(labels.cpu().numpy())\n",
        "\n",
        "# Step 11: Print the classification report\n",
        "predictions = [item for sublist in predictions for item in sublist]\n",
        "true_labels = [item for sublist in true_labels for item in sublist]\n",
        "print(classification_report(true_labels, predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGj_LDzk0Kac",
        "outputId": "638d40cc-43e3-4930-a604-d7ba4c032091"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      1.00      0.93         7\n",
            "           1       1.00      1.00      1.00         2\n",
            "           2       0.00      0.00      0.00         1\n",
            "           3       0.00      0.00      0.00         1\n",
            "           4       0.50      0.60      0.55         5\n",
            "           5       1.00      1.00      1.00         4\n",
            "           6       0.33      0.33      0.33         3\n",
            "           7       0.67      0.50      0.57         8\n",
            "           8       0.33      0.67      0.44         3\n",
            "           9       0.67      0.50      0.57         4\n",
            "\n",
            "    accuracy                           0.66        38\n",
            "   macro avg       0.54      0.56      0.54        38\n",
            "weighted avg       0.65      0.66      0.64        38\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate predictions\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = total_correct / total_samples * 100  # Convert to percentage\n",
        "\n",
        "    print(f'Epoch 5, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXObHNif0ovD",
        "outputId": "e58686e4-a413-42aa-c34c-8d7e9ec9ba76"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Loss: 0.5321, Accuracy: 96.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_category(text, model, tokenizer, max_length, device):\n",
        "    # Tokenize the input text\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Move tensors to the appropriate device\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient calculation for inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predicted_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    # Return the predicted class\n",
        "    return predicted_class\n"
      ],
      "metadata": {
        "id": "DdmB6uTm3DvF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_category = {\n",
        "    0:'Food & Drinks',\n",
        "    1:'Vehicle',\n",
        "    2:'Housing',\n",
        "    3:'Transportation',\n",
        "    4:'Health',\n",
        "    5:'Entertainment',\n",
        "    6:'Investment',\n",
        "    7:'Other',\n",
        "    8:'Financial Expenses',\n",
        "    9:'Shopping'\n",
        "}"
      ],
      "metadata": {
        "id": "6EAUfTw13FXf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(df):\n",
        "    X = df['expense_designation'].tolist()\n",
        "    y = df['id_category'].tolist()\n",
        "    return X, y\n",
        "\n",
        "def create_dataloaders(X, y, tokenizer, max_length, batch_size=16):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "    train_dataset = TextDataset(X_train, y_train, tokenizer, max_length)\n",
        "    val_dataset = TextDataset(X_val, y_val, tokenizer, max_length)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def train_model(model, train_loader, optimizer, device, num_epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            logits = outputs.logits\n",
        "            predicted_labels = torch.argmax(logits, dim=1)\n",
        "            correct += (predicted_labels == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = correct / total\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss}, Accuracy: {accuracy}')\n",
        "    return model\n",
        "\n",
        "def save_model(model, tokenizer, model_path, tokenizer_path):\n",
        "    model.save_pretrained(model_path)\n",
        "    tokenizer.save_pretrained(tokenizer_path)\n",
        "\n",
        "def load_model(model_path, tokenizer_path, num_labels, device):\n",
        "    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
        "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=num_labels)\n",
        "    model = model.to(device)\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "PMyNVc0431IW"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(model, tokenizer, '/content/classification_model/model', '/content/classification_model/tokenizer')\n"
      ],
      "metadata": {
        "id": "q7qkt_-a8jMy"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_model('/content/classification_model/model', '/content/classification_model/tokenizer', 10, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Fh8E2w3_OK1",
        "outputId": "efcef8e5-6dd9-4320-dee9-b41ec7457900"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(BertForSequenceClassification(\n",
              "   (bert): BertModel(\n",
              "     (embeddings): BertEmbeddings(\n",
              "       (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "       (position_embeddings): Embedding(512, 768)\n",
              "       (token_type_embeddings): Embedding(2, 768)\n",
              "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "       (dropout): Dropout(p=0.1, inplace=False)\n",
              "     )\n",
              "     (encoder): BertEncoder(\n",
              "       (layer): ModuleList(\n",
              "         (0-11): 12 x BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSdpaSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "             (intermediate_act_fn): GELUActivation()\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "       )\n",
              "     )\n",
              "     (pooler): BertPooler(\n",
              "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "       (activation): Tanh()\n",
              "     )\n",
              "   )\n",
              "   (dropout): Dropout(p=0.1, inplace=False)\n",
              "   (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
              " ),\n",
              " BertTokenizer(name_or_path='/content/classification_model/tokenizer', vocab_size=119547, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              " \t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " \t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " \t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " \t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " \t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " })"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, optimizer, device, num_epochs=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5VABUrF_e0n",
        "outputId": "0899bed2-b594-4c6c-8893-f110f66f816b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2, Loss: 0.3119432722980326, Accuracy: 0.9824561403508771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vBXoyVPy_vMR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}